"""
ë‹¤ë‚˜ì™€ ì˜¤ëŠ˜ì˜ íŠ¹ê°€ í˜ì´ì§€ í¬ë¡¤ëŸ¬
"""
import asyncio
import re
from typing import List, Optional, Dict, Any
from urllib.parse import urljoin

from playwright.async_api import async_playwright, Page, Browser
from loguru import logger

from app.models.schemas import SpecialProduct


class SpecialDealsCrawler:
    """ë‹¤ë‚˜ì™€ ì˜¤ëŠ˜ì˜ íŠ¹ê°€ í˜ì´ì§€ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.playwright = None
        self.base_url = "https://m.danawa.com"
        self.special_deals_url = "https://m.danawa.com/leftPanel/cmPick.html"
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=[
                '--disable-dev-shm-usage', 
                '--no-sandbox', 
                '--disable-gpu',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Mobile/15E148 Safari/604.1'
            ]
        )
        
        context = await self.browser.new_context(
            viewport={'width': 375, 'height': 667},  # ëª¨ë°”ì¼ ë·°í¬íŠ¸
            user_agent='Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Mobile/15E148 Safari/604.1',
            extra_http_headers={
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }
        )
        
        self.page = await context.new_page()
        self.page.set_default_timeout(60000)  # 60ì´ˆ
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        try:
            if self.page:
                await self.page.close()
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
        except Exception as e:
            logger.error(f"ë¸Œë¼ìš°ì € ì¢…ë£Œ ì˜¤ë¥˜: {e}")
    
    async def crawl_special_deals(self, max_products: int = 50) -> List[SpecialProduct]:
        """ë‹¤ë‚˜ì™€ ì˜¤ëŠ˜ì˜ íŠ¹ê°€ ìƒí’ˆ í¬ë¡¤ë§"""
        products = []
        
        try:
            logger.info(f"ğŸš€ ë‹¤ë‚˜ì™€ íŠ¹ê°€ í˜ì´ì§€ ì ‘ê·¼: {self.special_deals_url}")
            
            # íŠ¹ê°€ í˜ì´ì§€ë¡œ ì´ë™
            await self.page.goto(self.special_deals_url, wait_until='domcontentloaded', timeout=60000)
            await asyncio.sleep(3)
            logger.info("âœ… íŠ¹ê°€ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
            
            # í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ì½˜í…ì¸  ë¡œë“œ
            await self._scroll_to_load_content()
            
            # íŠ¹ê°€ ìƒí’ˆ ëª©ë¡ í¬ë¡¤ë§
            products = await self._extract_special_products(max_products)
            
            logger.info(f"ğŸ‰ ì´ {len(products)}ê°œì˜ íŠ¹ê°€ ìƒí’ˆì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            logger.error(f"âŒ íŠ¹ê°€ ìƒí’ˆ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            
        return products
    
    async def _scroll_to_load_content(self):
        """ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ì½˜í…ì¸  ë¡œë“œ"""
        try:
            logger.info("ğŸ“œ í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì¤‘...")
            for i in range(5):  # íŠ¹ê°€ ìƒí’ˆì´ ë§ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë” ë§ì´ ìŠ¤í¬ë¡¤
                await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(2)
            logger.info("âœ… ìŠ¤í¬ë¡¤ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë¡¤ ì˜¤ë¥˜: {e}")
    
    async def _extract_special_products(self, max_products: int) -> List[SpecialProduct]:
        """íŠ¹ê°€ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
        products = []
        
        try:
            # íŠ¹ê°€ ìƒí’ˆ ì»¨í…Œì´ë„ˆ ì„ íƒì (ë¬¸ì„œì—ì„œ ì œê³µëœ ê²½ë¡œ ê¸°ë°˜)
            container_selector = "#cmPick-category-container"
            
            # ì»¨í…Œì´ë„ˆê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            await self.page.wait_for_selector(container_selector, timeout=30000)
            
            # íŠ¹ê°€ ìƒí’ˆ ì•„ì´í…œë“¤ ì„ íƒ
            # ë¬¸ì„œì˜ ê²½ë¡œ: #cmPick-category-item-42124 > div > a
            # ì¼ë°˜í™”ëœ ì„ íƒì ì‚¬ìš©
            item_selector = f"{container_selector} [id*='cmPick-category-item-'] > div > a"
            
            # ëª¨ë“  íŠ¹ê°€ ìƒí’ˆ ë§í¬ ìˆ˜ì§‘
            product_links = await self.page.query_selector_all(item_selector)
            logger.info(f"ğŸ” ë°œê²¬ëœ íŠ¹ê°€ ìƒí’ˆ ë§í¬: {len(product_links)}ê°œ")
            
            # ìµœëŒ€ ê°œìˆ˜ë§Œí¼ ì²˜ë¦¬
            process_count = min(len(product_links), max_products)
            
            for i in range(process_count):
                try:
                    link = product_links[i]
                    
                    # ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
                    product_info = await self._extract_product_info_from_link(link, i)
                    
                    if product_info:
                        products.append(product_info)
                        logger.info(f"âœ… ìƒí’ˆ {i+1}/{process_count}: {product_info.product_name}")
                    
                    # ê° ìƒí’ˆ ì²˜ë¦¬ í›„ ì§§ì€ ëŒ€ê¸°
                    await asyncio.sleep(0.5)
                    
                except Exception as e:
                    logger.error(f"âŒ ìƒí’ˆ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
            
        except Exception as e:
            logger.error(f"âŒ íŠ¹ê°€ ìƒí’ˆ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return products
    
    async def _extract_product_info_from_link(self, link_element, index: int) -> Optional[SpecialProduct]:
        """ê°œë³„ ìƒí’ˆ ë§í¬ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ìƒí’ˆ URL ì¶”ì¶œ
            product_url = await link_element.get_attribute('href')
            if not product_url:
                return None
            
            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if product_url.startswith('/'):
                product_url = urljoin(self.base_url, product_url)
            
            # ìƒí’ˆ ID ì¶”ì¶œ (URLì—ì„œ)
            product_id = self._extract_product_id_from_url(product_url)
            if not product_id:
                product_id = f"special_{index}_{int(asyncio.get_event_loop().time())}"
            
            # ìƒí’ˆ ì´ë¯¸ì§€ URL ì¶”ì¶œ (êµ¬ì²´ì ì¸ ì„ íƒì ì‚¬ìš©)
            image_url = None
            img_element = await link_element.query_selector("div.box__thumbnail > img")
            if img_element:
                image_url = await img_element.get_attribute('src')
                if image_url and image_url.startswith('/'):
                    image_url = urljoin(self.base_url, image_url)
                elif image_url and image_url.startswith('//'):
                    image_url = f"https:{image_url}"
            
            # ìƒí’ˆëª… ì¶”ì¶œ (êµ¬ì²´ì ì¸ ì„ íƒì ì‚¬ìš©)
            product_name = None
            title_element = await link_element.query_selector("div.box__info > div.box__title")
            if title_element:
                product_name = await title_element.inner_text()
                product_name = product_name.strip() if product_name else None
            
            # ê°€ê²© ì •ë³´ ì¶”ì¶œ (êµ¬ì²´ì ì¸ ì„ íƒì ì‚¬ìš©)
            price_info = await self._extract_detailed_price_info(link_element)
            
            return SpecialProduct(
                product_id=product_id,
                product_name=product_name or f"íŠ¹ê°€ìƒí’ˆ {index+1}",
                product_url=product_url,
                image_url=image_url,
                price=price_info.get('price'),
                original_price=price_info.get('original_price'),
                discount_rate=price_info.get('discount_rate'),
                brand=None,  # íŠ¹ê°€ í˜ì´ì§€ì—ì„œëŠ” ë¸Œëœë“œ ì •ë³´ê°€ ì œí•œì 
                category="íŠ¹ê°€ìƒí’ˆ",
                rating=None,
                review_count=0,
                is_crawled=False
            )
            
        except Exception as e:
            logger.error(f"âŒ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def _extract_product_id_from_url(self, url: str) -> Optional[str]:
        """URLì—ì„œ ìƒí’ˆ ID ì¶”ì¶œ"""
        try:
            # code íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            code_match = re.search(r'[?&]code=([^&]+)', url)
            if code_match:
                return code_match.group(1)
            
            # pcode íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            pcode_match = re.search(r'[?&]pcode=([^&]+)', url)
            if pcode_match:
                return pcode_match.group(1)
            
            # URL ê²½ë¡œì—ì„œ ìˆ«ì ì¶”ì¶œ
            path_match = re.search(r'/(\d+)/?', url)
            if path_match:
                return path_match.group(1)
            
            return None
        except:
            return None
    
    async def _extract_text_from_element(self, element, description: str) -> Optional[str]:
        """ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            text = await element.inner_text()
            return text.strip() if text else None
        except:
            return None
    
    async def _extract_price_info(self, element) -> Dict[str, Optional[str]]:
        """ê°€ê²© ì •ë³´ ì¶”ì¶œ (ê¸°ì¡´ ë°©ì‹)"""
        price_info = {
            'price': None,
            'original_price': None,
            'discount_rate': None
        }
        
        try:
            # ê°€ê²© ê´€ë ¨ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
            text_content = await element.inner_text()
            
            # ì› ë‹¨ìœ„ ê°€ê²© ì¶”ì¶œ
            price_matches = re.findall(r'[\d,]+ì›', text_content)
            if price_matches:
                price_info['price'] = price_matches[0]
                if len(price_matches) > 1:
                    price_info['original_price'] = price_matches[1]
            
            # í• ì¸ìœ¨ ì¶”ì¶œ
            discount_match = re.search(r'(\d+)%', text_content)
            if discount_match:
                price_info['discount_rate'] = f"{discount_match.group(1)}%"
                
        except Exception as e:
            logger.debug(f"ê°€ê²© ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return price_info
    
    async def _extract_detailed_price_info(self, element) -> Dict[str, Optional[str]]:
        """êµ¬ì²´ì ì¸ ì„ íƒìë¥¼ ì‚¬ìš©í•œ ê°€ê²© ì •ë³´ ì¶”ì¶œ"""
        price_info = {
            'price': None,
            'original_price': None,
            'discount_rate': None
        }
        
        try:
            # ê°€ê²© ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            price_container = await element.query_selector("div.box__info > div.box__price")
            if not price_container:
                # ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                return await self._extract_price_info(element)
            
            # ê°€ê²© ì»¨í…Œì´ë„ˆ ë‚´ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            price_text = await price_container.inner_text()
            
            # í• ì¸ìœ¨ ì¶”ì¶œ
            discount_match = re.search(r'(\d+)%', price_text)
            if discount_match:
                price_info['discount_rate'] = f"{discount_match.group(1)}%"
            
            # ê°€ê²© ì •ë³´ ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
            price_matches = re.findall(r'[\d,]+ì›', price_text)
            if price_matches:
                if len(price_matches) >= 2:
                    # ì²« ë²ˆì§¸ê°€ í• ì¸ ê°€ê²©, ë‘ ë²ˆì§¸ê°€ ì›ë˜ ê°€ê²©ì¸ ê²½ìš°ê°€ ë§ìŒ
                    price_info['price'] = price_matches[0]
                    price_info['original_price'] = price_matches[1]
                else:
                    price_info['price'] = price_matches[0]
            
            # ë” êµ¬ì²´ì ì¸ ì„ íƒìë¡œ ì‹œë„
            if not price_info['price']:
                # í˜„ì¬ ê°€ê²© ì°¾ê¸°
                current_price_elem = await price_container.query_selector("span.price, .price_current, .price-current")
                if current_price_elem:
                    current_price_text = await current_price_elem.inner_text()
                    price_match = re.search(r'([\d,]+ì›)', current_price_text)
                    if price_match:
                        price_info['price'] = price_match.group(1)
                
                # ì›ë˜ ê°€ê²© ì°¾ê¸°
                original_price_elem = await price_container.query_selector("span.price_origin, .price-origin, .original-price")
                if original_price_elem:
                    original_price_text = await original_price_elem.inner_text()
                    price_match = re.search(r'([\d,]+ì›)', original_price_text)
                    if price_match:
                        price_info['original_price'] = price_match.group(1)
                
        except Exception as e:
            logger.debug(f"ìƒì„¸ ê°€ê²© ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´
            return await self._extract_price_info(element)
        
        return price_info


async def crawl_special_deals(max_products: int = 50) -> List[SpecialProduct]:
    """ë‹¤ë‚˜ì™€ íŠ¹ê°€ ìƒí’ˆ í¬ë¡¤ë§ í•¨ìˆ˜"""
    async with SpecialDealsCrawler() as crawler:
        return await crawler.crawl_special_deals(max_products) 