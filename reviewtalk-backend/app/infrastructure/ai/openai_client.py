"""
OpenAI GPT, Google Gemini, Qwen3 ë“±ì„ ì‚¬ìš©í•œ AI ì‘ë‹µ ìƒì„±
"""
from typing import List, Dict, Any, Optional
from openai import OpenAI
import google.generativeai as genai
from app.core.config import settings
import logging

logger = logging.getLogger(__name__)

class AIClient:
    """OpenAI GPT, Google Gemini, Qwen3ë¥¼ ì§€ì›í•˜ëŠ” AI ì‘ë‹µ ìƒì„± í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        """AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.provider = settings.llm_provider
        
        if self.provider == "openai":
            self.client = OpenAI(api_key=settings.openai_api_key)
            self.model = settings.openai_model
            logger.info(f"[AIClient.__init__] OpenAI ëª¨ë¸: {self.model}, API KEY ì¡´ì¬ ì—¬ë¶€: {bool(settings.openai_api_key)}")
        elif self.provider == "gemini":
            genai.configure(api_key=settings.gemini_api_key)
            self.model = settings.gemini_model
            logger.info(f"[AIClient.__init__] Gemini ëª¨ë¸: {self.model}, API KEY ì¡´ì¬ ì—¬ë¶€: {bool(settings.gemini_api_key)}")
        elif self.provider in ["qwen3", "local"]:
            # Qwen3 ë˜ëŠ” ë¡œì»¬ LLM (Ollama, vLLM ë“±)ì„ OpenAI í˜¸í™˜ APIë¡œ ì‚¬ìš©
            self.client = OpenAI(
                base_url=settings.local_llm_base_url,
                api_key=settings.local_llm_api_key
            )
            self.model = settings.local_llm_model
            logger.info(f"[AIClient.__init__] ë¡œì»¬ LLM ëª¨ë¸: {self.model}, Base URL: {settings.local_llm_base_url}")
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” LLM ì œê³µì—…ì²´: {self.provider}")

    def _generate_openai_response(self, system_prompt: str, user_prompt: str, temperature: float = 0.3, max_tokens: int = 1000) -> str:
        """OpenAI APIë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"[_generate_openai_response] OpenAI API í˜¸ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            raise

    def _generate_local_llm_response(self, system_prompt: str, user_prompt: str, temperature: float = 0.3, max_tokens: int = 1000) -> str:
        """ë¡œì»¬ LLM (Qwen3, Ollama ë“±) OpenAI í˜¸í™˜ APIë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"[_generate_local_llm_response] ë¡œì»¬ LLM API í˜¸ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            raise

    def _generate_gemini_response(self, system_prompt: str, user_prompt: str, temperature: float = 0.3, max_tokens: int = 1000) -> str:
        """Google Gemini APIë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±"""
        try:
            model = genai.GenerativeModel(
                model_name=self.model,
                generation_config=genai.types.GenerationConfig(
                    temperature=temperature,
                    max_output_tokens=max_tokens,
                )
            )
            
            # GeminiëŠ” system instructionê³¼ user promptë¥¼ ê²°í•©í•´ì„œ ì‚¬ìš©
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            response = model.generate_content(full_prompt)
            return response.text
        except Exception as e:
            logger.error(f"[_generate_gemini_response] Gemini API í˜¸ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            raise

    def generate_response(self, system_prompt: str, user_prompt: str, temperature: float = 0.3, max_tokens: int = 1000) -> str:
        """ì„ íƒëœ LLM ì œê³µì—…ì²´ë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±"""
        if self.provider == "openai":
            return self._generate_openai_response(system_prompt, user_prompt, temperature, max_tokens)
        elif self.provider == "gemini":
            return self._generate_gemini_response(system_prompt, user_prompt, temperature, max_tokens)
        elif self.provider in ["qwen3", "local"]:
            return self._generate_local_llm_response(system_prompt, user_prompt, temperature, max_tokens)
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” LLM ì œê³µì—…ì²´: {self.provider}")

    def generate_review_summary(
        self, 
        reviews: List[Dict[str, Any]], 
        user_question: str,
        recent_conversations: List[Dict[str, Any]] = None
    ) -> str:
        """ë¦¬ë·° ë°ì´í„°ì™€ ìµœê·¼ ëŒ€í™” ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        logger.info(f"[generate_review_summary] í˜¸ì¶œ - user_question: {user_question}")
        logger.info(f"[generate_review_summary] reviews ê°œìˆ˜: {len(reviews)}")
        logger.info(f"[generate_review_summary] recent_conversations ê°œìˆ˜: {len(recent_conversations) if recent_conversations else 0}")
        logger.info(f"[generate_review_summary] ì‚¬ìš© ì¤‘ì¸ LLM: {self.provider} ({self.model})")
        
        # ìµœê·¼ ëŒ€í™” ë§¥ë½ ì¤€ë¹„
        conversation_context = ""
        if recent_conversations:
            conversation_context = "\n\n".join([
                f"[{conv.get('chat_user_id', '')}] {conv.get('message', '')}" for conv in recent_conversations
            ])
            conversation_context = f"\n\n[ìµœê·¼ ëŒ€í™” ë§¥ë½]\n{conversation_context}"
        # ë¦¬ë·° í…ìŠ¤íŠ¸ ì¤€ë¹„
        review_texts = []
        for review in reviews:
            document = review.get("document", "")
            metadata = review.get("metadata", {})
            rating = metadata.get("rating", "N/A")
            date = metadata.get("date", "N/A")
            
            review_text = f"[í‰ì : {rating}, ë‚ ì§œ: {date}]\n{document}"
            review_texts.append(review_text)
        
        reviews_context = "\n\n".join(review_texts)
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        system_prompt = """

## ì—­í• 
-ë‹¹ì‹ ì€ 'ë¦¬ë·°í†¡'ì˜ ìƒí’ˆ ë¦¬ë·° ë¶„ì„ ì „ë¬¸ AI ì±—ë´‡ì…ë‹ˆë‹¤.
## ì‘ë‹µ ìŠ¤íƒ€ì¼
- ì¹œì ˆí•˜ê³  ì‹ ë¢°ê° ìˆëŠ” ì¡´ëŒ“ë§ ì‚¬ìš©
- 100~200ì ë‚´ì™¸ì˜ ê°„ê²°í•˜ê³  ëª…í™•í•œ ì‘ë‹µ
- ë‹¤ìŒ í‘œí˜„ë“¤ì„ ìì£¼ ì‚¬ìš©í•˜ì„¸ìš”:
  - "ë¦¬ë·°ë¥¼ ë¶„ì„í•´ë³´ë‹ˆâ€¦"
  - "êµ¬ë§¤í•˜ì‹  ë¶„ë“¤ ì˜ê²¬ì„ ë³´ë©´â€¦"
  

## ì‘ë‹µ êµ¬ì¡°
1. **ê´€ë ¨ ë¦¬ë·° ìˆ˜ ìš”ì•½**
   - ì˜ˆ: "ì „ì²´ 1,500ê°œì˜ ë¦¬ë·° ì¤‘ 120ëª…ì´ ì°©ìš©ê°ì— ëŒ€í•´ ì–¸ê¸‰í–ˆì–´ìš”."
2. **ë¦¬ë·° ë¶„ì„ ê²°ê³¼ ìš”ì•½** (í•œ ë¬¸ì¥)
3. **ì‹¤ì œ ë¦¬ë·° ë‚´ìš© ì¸ìš©** (ì‘ì„±ì¼, í‰ì  í¬í•¨í•˜ì—¬ 1~2ê°œ ì„œìˆ í˜• ì¸ìš©)
   - ì˜ˆ:
     [í‰ì : â˜…â˜…â˜…â˜…â˜…,  "ì´ì–´í° ì°©ìš©ê°ì´ ë§¤ìš° ì¢‹ì•„ìš”."
     [í‰ì : â˜…â˜†â˜†â˜†â˜†,  "ë°˜í’ˆ ì œí’ˆì´ ì˜¨ ê²ƒ ê°™ì•„ ì‹¤ë§í–ˆìŠµë‹ˆë‹¤."
   :ì˜¤ë¥¸ìª½ì„_ê°€ë¦¬í‚¤ëŠ”_ì†_ëª¨ì–‘: í‘œ í˜•íƒœëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì¤„ë°”ê¿ˆê³¼ ì¸ìš©ë¶€í˜¸ë¥¼ í™œìš©í•œ ì„œìˆ í˜• ì¸ìš©ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
4. **ê¸/ë¶€ì • ìš”ì•½ ë° ê²°ë¡  ì œì‹œ**
## ì˜ˆì™¸ ìƒí™© ëŒ€ì‘
- ê´€ë ¨ ë¦¬ë·° ì—†ìŒ: "ì£„ì†¡í•´ìš”, í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•œ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë„¤ìš” :ë•€_í˜ë¦¬ëŠ”_ì›ƒëŠ”_ì–¼êµ´:"
- ì˜ê²¬ì´ ë‚˜ë‰˜ëŠ” ê²½ìš°: "ì˜ê²¬ì´ ë‚˜ë‰˜ëŠ” ë¶€ë¶„ì´ì—ìš”. ê¸ì •ì ìœ¼ë¡œëŠ”â€¦, ë°˜ëŒ€ë¡œëŠ”â€¦"
- ì œí’ˆ ì™¸ ì§ˆë¬¸: "ìƒí’ˆ ë¦¬ë·°ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆì–´ìš” :ë¯¸ì†Œì§“ëŠ”_ìƒê¸°ëœ_ì–¼êµ´:"
## ì£¼ì˜ì‚¬í•­
- ë¦¬ë·°ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ë¡ í•˜ê±°ë‚˜ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
- ê°ì •ì /ê´‘ê³ ì„± í‘œí˜„ì„ í”¼í•˜ê³ , ì¤‘ë¦½ì ì´ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
- í•œë‘ ë¦¬ë·°ë§Œì„ ê·¼ê±°ë¡œ ì¼ë°˜í™”í•˜ì§€ ë§ˆì„¸ìš”. ë°˜ë“œì‹œ ë³µìˆ˜ì˜ ë¦¬ë·°ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
- ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸°ê³„ì ì¸ ë‹µë³€ì„ í”¼í•˜ê³ , ì‚¬ìš©ìê°€ ì‹ ë¢°í•  ìˆ˜ ìˆë„ë¡ ì„œìˆ í˜•ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”."""
        user_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {user_question}\n\n{conversation_context}\n\nê´€ë ¨ ë¦¬ë·° ë°ì´í„°:\n{reviews_context}\n\nìœ„ ë¦¬ë·° ë°ì´í„°ì™€ ìµœê·¼ ëŒ€í™” ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""
        logger.info(f"[generate_review_summary] system_prompt ê¸¸ì´: {len(system_prompt)}")
        logger.info(f"[generate_review_summary] user_prompt ê¸¸ì´: {len(user_prompt)}")
        
        try:
            response = self.generate_response(system_prompt, user_prompt, temperature=0.3, max_tokens=1000)
            logger.info(f"[generate_review_summary] AI ì‘ë‹µ ìˆ˜ì‹  - ì‘ë‹µ ê¸¸ì´: {len(response) if response else 0}")
            return response
        except Exception as e:
            logger.error(f"[generate_review_summary] AI API í˜¸ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def generate_product_overview(self, reviews: List[Dict[str, Any]]) -> str:
        """ì œí’ˆ ì „ì²´ ë¦¬ë·° ìš”ì•½ ìƒì„±"""
        logger.info(f"[generate_product_overview] í˜¸ì¶œ - ë¦¬ë·° ê°œìˆ˜: {len(reviews)}")
        logger.info(f"[generate_product_overview] ì‚¬ìš© ì¤‘ì¸ LLM: {self.provider} ({self.model})")
        
        # ë¦¬ë·° í†µê³„ ê³„ì‚°
        total_reviews = len(reviews)
        ratings = []
        review_texts = []
        for review in reviews:
            metadata = review.get("metadata", {})
            rating = metadata.get("rating")
            if rating and isinstance(rating, (int, float)):
                ratings.append(rating)
            document = review.get("document", "")
            review_texts.append(document)
        avg_rating = sum(ratings) / len(ratings) if ratings else 0
        reviews_sample = "\n\n".join(review_texts[:10])  # ìµœëŒ€ 10ê°œ ë¦¬ë·°ë§Œ ì‚¬ìš©
        logger.info(f"[generate_product_overview] í‰ê·  í‰ì : {avg_rating:.2f}, ìƒ˜í”Œ ë¦¬ë·° ê°œìˆ˜: {len(review_texts[:10])}")
        system_prompt = """
## ì—­í• 
-ë‹¹ì‹ ì€ 'ë¦¬ë·°í†¡'ì˜ ìƒí’ˆ ë¦¬ë·° ë¶„ì„ ì „ë¬¸ AI ì±—ë´‡ì…ë‹ˆë‹¤.

## ì‘ë‹µ ìŠ¤íƒ€ì¼
- ì¹œì ˆí•˜ê³  ì‹ ë¢°ê° ìˆëŠ” ì¡´ëŒ“ë§ ì‚¬ìš©
- 100~200ì ë‚´ì™¸ì˜ ê°„ê²°í•˜ê³  ëª…í™•í•œ ì‘ë‹µ
- ë‹¤ìŒ í‘œí˜„ë“¤ì„ ìì£¼ ì‚¬ìš©í•˜ì„¸ìš”:
  - "ë¦¬ë·°ë¥¼ ë¶„ì„í•´ë³´ë‹ˆâ€¦"
  - "êµ¬ë§¤í•˜ì‹  ë¶„ë“¤ ì˜ê²¬ì„ ë³´ë©´â€¦"
  - "ëŒ€ë¶€ë¶„ì˜ ì‚¬ìš©ìë“¤ì´â€¦"
- ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì„¸ìš”: ğŸ˜Š ğŸ™ âš ï¸ ğŸ’¬ ğŸ‘ ğŸ’¡
## ì‘ë‹µ êµ¬ì¡°
1. **ê´€ë ¨ ë¦¬ë·° ìˆ˜ ìš”ì•½**
   - ì˜ˆ: "ì „ì²´ 1,500ê°œì˜ ë¦¬ë·° ì¤‘ 120ëª…ì´ ì°©ìš©ê°ì— ëŒ€í•´ ì–¸ê¸‰í–ˆì–´ìš”."
2. **ë¦¬ë·° ë¶„ì„ ê²°ê³¼ ìš”ì•½** (í•œ ë¬¸ì¥)
3. **ì‹¤ì œ ë¦¬ë·° ë‚´ìš© ì¸ìš©** (ì‘ì„±ì¼, í‰ì  í¬í•¨í•˜ì—¬ 1~2ê°œ ì„œìˆ í˜• ì¸ìš©)
   - ì˜ˆ:
     [í‰ì : â˜…â˜…â˜…â˜…â˜…,  "ì´ì–´í° ì°©ìš©ê°ì´ ë§¤ìš° ì¢‹ì•„ìš”."
     [í‰ì : â˜…â˜†â˜†â˜†â˜†,  "ë°˜í’ˆ ì œí’ˆì´ ì˜¨ ê²ƒ ê°™ì•„ ì‹¤ë§í–ˆìŠµë‹ˆë‹¤."
   :ì˜¤ë¥¸ìª½ì„_ê°€ë¦¬í‚¤ëŠ”_ì†_ëª¨ì–‘: í‘œ í˜•íƒœëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì¤„ë°”ê¿ˆê³¼ ì¸ìš©ë¶€í˜¸ë¥¼ í™œìš©í•œ ì„œìˆ í˜• ì¸ìš©ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
4. **ê¸/ë¶€ì • ìš”ì•½ ë° ê²°ë¡  ì œì‹œ**
## ì˜ˆì™¸ ìƒí™© ëŒ€ì‘
- ê´€ë ¨ ë¦¬ë·° ì—†ìŒ: "ì£„ì†¡í•´ìš”, í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•œ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë„¤ìš” :ë•€_í˜ë¦¬ëŠ”_ì›ƒëŠ”_ì–¼êµ´:"
- ì˜ê²¬ì´ ë‚˜ë‰˜ëŠ” ê²½ìš°: "ì˜ê²¬ì´ ë‚˜ë‰˜ëŠ” ë¶€ë¶„ì´ì—ìš”. ê¸ì •ì ìœ¼ë¡œëŠ”â€¦, ë°˜ëŒ€ë¡œëŠ”â€¦"
- ì œí’ˆ ì™¸ ì§ˆë¬¸: "ìƒí’ˆ ë¦¬ë·°ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆì–´ìš” :ë¯¸ì†Œì§“ëŠ”_ìƒê¸°ëœ_ì–¼êµ´:"
## ì£¼ì˜ì‚¬í•­
- ë¦¬ë·°ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ë¡ í•˜ê±°ë‚˜ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
- ê°ì •ì /ê´‘ê³ ì„± í‘œí˜„ì„ í”¼í•˜ê³ , ì¤‘ë¦½ì ì´ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
- í•œë‘ ë¦¬ë·°ë§Œì„ ê·¼ê±°ë¡œ ì¼ë°˜í™”í•˜ì§€ ë§ˆì„¸ìš”. ë°˜ë“œì‹œ ë³µìˆ˜ì˜ ë¦¬ë·°ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
- ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸°ê³„ì ì¸ ë‹µë³€ì„ í”¼í•˜ê³ , ì‚¬ìš©ìê°€ ì‹ ë¢°í•  ìˆ˜ ìˆë„ë¡ ì„œìˆ í˜•ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”."""
        user_prompt = f"""ì´ {total_reviews}ê°œì˜ ë¦¬ë·° (í‰ê·  í‰ì : {avg_rating:.1f}/5.0)\n\nëŒ€í‘œ ë¦¬ë·°ë“¤:\n{reviews_sample}\n\nìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ ì œí’ˆì— ëŒ€í•œ ì¢…í•©ì ì¸ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”."""
        logger.info(f"[generate_product_overview] system_prompt ê¸¸ì´: {len(system_prompt)}")
        logger.info(f"[generate_product_overview] user_prompt ê¸¸ì´: {len(user_prompt)}")
        try:
            response = self.generate_response(system_prompt, user_prompt, temperature=0.7, max_tokens=800)
            logger.info(f"[generate_product_overview] AI ì‘ë‹µ ìˆ˜ì‹  - ì‘ë‹µ ê¸¸ì´: {len(response) if response else 0}")
            return response
        except Exception as e:
            logger.error(f"[generate_product_overview] AI API í˜¸ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            return "ì œí’ˆ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


# ì „ì—­ AI í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ - ì§€ì—° ì´ˆê¸°í™”
ai_client = None

def get_ai_client():
    """AI í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global ai_client
    if ai_client is None:
        ai_client = AIClient()
    return ai_client

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ OpenAI í´ë¼ì´ì–¸íŠ¸ ë³„ì¹­
OpenAIClient = AIClient
get_openai_client = get_ai_client 