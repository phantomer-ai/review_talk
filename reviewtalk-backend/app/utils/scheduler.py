"""
ìë™ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ - ë§¤ì¼ íŠ¹ê°€ ìƒí’ˆ í¬ë¡¤ë§
"""
import asyncio
import schedule
import time
from datetime import datetime
from threading import Thread

from loguru import logger
from app.models.schemas import CrawlSpecialProductsRequest
from app.services.special_deals_service import special_deals_service


class CrawlingScheduler:
    """ìë™ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self):
        self.is_running = False
        self.scheduler_thread = None
    
    async def daily_special_deals_crawling(self):
        """ë§¤ì¼ ì‹¤í–‰ë˜ëŠ” íŠ¹ê°€ ìƒí’ˆ í¬ë¡¤ë§ ì‘ì—…"""
        logger.info("ğŸ• [ìŠ¤ì¼€ì¤„ëŸ¬] ì¼ì¼ íŠ¹ê°€ ìƒí’ˆ í¬ë¡¤ë§ ì‹œì‘")
        
        try:
            # 1. íŠ¹ê°€ ìƒí’ˆ ëª©ë¡ í¬ë¡¤ë§ (ë¦¬ë·°ë„ í•¨ê»˜ ìˆ˜ì§‘)
            request = CrawlSpecialProductsRequest(
                max_products=50,
                crawl_reviews=True,  # ë¦¬ë·°ë„ í•¨ê»˜ í¬ë¡¤ë§ìœ¼ë¡œ ë³€ê²½
                max_reviews_per_product=100
            )
            
            result = await special_deals_service.crawl_and_save_special_deals(request)
            
            if result.success:
                logger.info(f"âœ… [ìŠ¤ì¼€ì¤„ëŸ¬] íŠ¹ê°€ ìƒí’ˆ {result.total_products}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                
                # 2. ê¸°ì¡´ ë¯¸í¬ë¡¤ë§ ìƒí’ˆë“¤ì˜ ë¦¬ë·° ë°°ì¹˜ ì²˜ë¦¬
                batch_result = await special_deals_service.process_uncrawled_products(batch_size=10)
                
                if batch_result.get("success"):
                    logger.info(f"âœ… [ìŠ¤ì¼€ì¤„ëŸ¬] ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {batch_result.get('processed_count', 0)}ê°œ ìƒí’ˆ")
                else:
                    logger.warning(f"âš ï¸ [ìŠ¤ì¼€ì¤„ëŸ¬] ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {batch_result.get('error_message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                
                # 3. ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ (7ì¼ ì´ì „)
                cleanup_result = special_deals_service.cleanup_old_products(days=7)
                if cleanup_result.get("success"):
                    logger.info(f"âœ… [ìŠ¤ì¼€ì¤„ëŸ¬] ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬: {cleanup_result.get('deleted_count', 0)}ê°œ ì‚­ì œ")
                
            else:
                logger.error(f"âŒ [ìŠ¤ì¼€ì¤„ëŸ¬] íŠ¹ê°€ ìƒí’ˆ í¬ë¡¤ë§ ì‹¤íŒ¨: {result.error_message}")
                
        except Exception as e:
            logger.error(f"âŒ [ìŠ¤ì¼€ì¤„ëŸ¬] ì¼ì¼ í¬ë¡¤ë§ ì‘ì—… ì˜¤ë¥˜: {e}")
        
        logger.info("ğŸ [ìŠ¤ì¼€ì¤„ëŸ¬] ì¼ì¼ íŠ¹ê°€ ìƒí’ˆ í¬ë¡¤ë§ ì™„ë£Œ")
    
    async def background_review_crawling(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” ë¦¬ë·° í¬ë¡¤ë§ ì‘ì—… (2ì‹œê°„ë§ˆë‹¤)"""
        logger.info("ğŸ”„ [ìŠ¤ì¼€ì¤„ëŸ¬] ë°±ê·¸ë¼ìš´ë“œ ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘")
        
        try:
            # ë¯¸í¬ë¡¤ë§ ìƒí’ˆë“¤ì˜ ë¦¬ë·°ë¥¼ ì†ŒëŸ‰ì”© ì²˜ë¦¬
            batch_result = await special_deals_service.process_uncrawled_products(batch_size=5)
            
            if batch_result.get("success"):
                processed = batch_result.get('processed_count', 0)
                if processed > 0:
                    logger.info(f"âœ… [ìŠ¤ì¼€ì¤„ëŸ¬] ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì™„ë£Œ: {processed}ê°œ ìƒí’ˆ")
                else:
                    logger.info("â„¹ï¸ [ìŠ¤ì¼€ì¤„ëŸ¬] ì²˜ë¦¬í•  ë¯¸í¬ë¡¤ë§ ìƒí’ˆ ì—†ìŒ")
            else:
                logger.warning(f"âš ï¸ [ìŠ¤ì¼€ì¤„ëŸ¬] ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {batch_result.get('error_message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                
        except Exception as e:
            logger.error(f"âŒ [ìŠ¤ì¼€ì¤„ëŸ¬] ë°±ê·¸ë¼ìš´ë“œ ë¦¬ë·° í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        
        logger.info("ğŸ [ìŠ¤ì¼€ì¤„ëŸ¬] ë°±ê·¸ë¼ìš´ë“œ ë¦¬ë·° í¬ë¡¤ë§ ì™„ë£Œ")
    
    def schedule_daily_jobs(self):
        """ì¼ì¼ ì‘ì—… ìŠ¤ì¼€ì¤„ ì„¤ì •"""
        # ë§¤ì¼ ì˜¤ì „ 9ì‹œì— íŠ¹ê°€ ìƒí’ˆ í¬ë¡¤ë§ (ë¦¬ë·° í¬í•¨)
        schedule.every().day.at("09:00").do(self._run_async_job, self.daily_special_deals_crawling)
        

        logger.info("ğŸ“… [ìŠ¤ì¼€ì¤„ëŸ¬] ì‘ì—… ìŠ¤ì¼€ì¤„ ì„¤ì • ì™„ë£Œ")
        logger.info("   - ë§¤ì¼ 09:00: íŠ¹ê°€ ìƒí’ˆ í¬ë¡¤ë§ (ë¦¬ë·° í¬í•¨)")

    def _run_async_job(self, async_func):
        """ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸° ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ì‹¤í–‰"""
        try:
            # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ì‹¤í–‰
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(async_func())
            loop.close()
        except Exception as e:
            logger.error(f"âŒ [ìŠ¤ì¼€ì¤„ëŸ¬] ë¹„ë™ê¸° ì‘ì—… ì‹¤í–‰ ì˜¤ë¥˜: {e}")
    
    def start_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        if self.is_running:
            logger.warning("âš ï¸ [ìŠ¤ì¼€ì¤„ëŸ¬] ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
            return
        
        self.is_running = True
        self.schedule_daily_jobs()
        
        def run_scheduler():
            logger.info("ğŸš€ [ìŠ¤ì¼€ì¤„ëŸ¬] ì‹œì‘ë¨")
            while self.is_running:
                try:
                    schedule.run_pending()
                    time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
                except Exception as e:
                    logger.error(f"âŒ [ìŠ¤ì¼€ì¤„ëŸ¬] ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                    time.sleep(60)
            logger.info("ğŸ›‘ [ìŠ¤ì¼€ì¤„ëŸ¬] ì¢…ë£Œë¨")
        
        self.scheduler_thread = Thread(target=run_scheduler, daemon=True)
        self.scheduler_thread.start()
        
        logger.info("âœ… [ìŠ¤ì¼€ì¤„ëŸ¬] ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    def stop_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
        if not self.is_running:
            logger.warning("âš ï¸ [ìŠ¤ì¼€ì¤„ëŸ¬] ì‹¤í–‰ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤")
            return
        
        self.is_running = False
        schedule.clear()
        
        if self.scheduler_thread and self.scheduler_thread.is_alive():
            self.scheduler_thread.join(timeout=5)
        
        logger.info("ğŸ›‘ [ìŠ¤ì¼€ì¤„ëŸ¬] ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    def get_scheduler_status(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ì¡°íšŒ"""
        next_runs = []
        for job in schedule.get_jobs():
            next_runs.append({
                "job": str(job.job_func),
                "next_run": job.next_run.strftime("%Y-%m-%d %H:%M:%S") if job.next_run else None
            })
        
        return {
            "is_running": self.is_running,
            "scheduled_jobs": len(schedule.get_jobs()),
            "next_runs": next_runs
        }


# ì „ì—­ ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
crawler_scheduler = CrawlingScheduler()


def init_scheduler():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”"""
    try:
        crawler_scheduler.start_scheduler()
        logger.info("âœ… ìë™ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")
    except Exception as e:
        logger.error(f"âŒ ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")


def shutdown_scheduler():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ì •ë¦¬"""
    try:
        crawler_scheduler.stop_scheduler()
        logger.info("âœ… ìë™ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
    except Exception as e:
        logger.error(f"âŒ ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ ì‹¤íŒ¨: {e}") 